{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW20gD2bWXyu"
      },
      "source": [
        "# Coursework 2 - Articial Intelligence and Machine Learning\n",
        "# Practical Component\n",
        "# Student Name: Nourhan Moustafa Mahmoud Mohamed El-Bagoury\n",
        "# Student Email: NMOHAM301@caledonian.ac.uk\n",
        "\n",
        "I confirm that the material contained within the submitted coursework is all my own work unless otherwise stated below.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IdY-OhzjWXyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73b44f1-06f5-478a-bf11-f0ccb08e14b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.1/110.1 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.8/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from wordcloud) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from wordcloud) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow\n",
        "! pip install contractions\n",
        "! pip install wordcloud\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gKNiMUh_WXyy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import re, string, unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "import contractions\n",
        "import wordcloud\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        "    Bidirectional,\n",
        "    Dense,\n",
        "    Embedding,\n",
        ")\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "import sklearn.preprocessing  \n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc3QDr3-WXyz"
      },
      "source": [
        "# 1. Data preparation\n",
        "\n",
        "### 1.1 Loading the data as a csv file using the pandas library into a dataframe named 'data', then printing out the top 10 raws of the loaded file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2FJH950sWXy0",
        "outputId": "8b34f375-2c7f-4715-886d-ad32fbc28cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-96c2d94ba521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweets.csv'"
          ]
        }
      ],
      "source": [
        "#load the data\n",
        "data = pd.read_csv(\"tweets.csv\")\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX1MmM5fWXy0"
      },
      "source": [
        "etting the shape of the data to show the number of colums and rows provided within the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFsarVcJWXy1"
      },
      "outputs": [],
      "source": [
        "# Number of colums & rows in the dataset\n",
        "print('The dataset has {} rows and {} colums'.format(data.shape[0], data.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvOlL4-DWXy1"
      },
      "outputs": [],
      "source": [
        "# Printing another general overview of the file data types.\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiKQf6dnWXy1"
      },
      "outputs": [],
      "source": [
        "# Type of the data in each colum\n",
        "DataType = data.dtypes\n",
        "print('Data type of each column:')\n",
        "print(DataType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxb5FPTTWXy2"
      },
      "outputs": [],
      "source": [
        "data.isnull(). sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Av_AVHWXy2"
      },
      "outputs": [],
      "source": [
        "data.duplicated(). sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJhsUSxcWXy2"
      },
      "outputs": [],
      "source": [
        "# Drop the duplicate rows\n",
        "data.drop_duplicates(keep=False,inplace=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U1r2CMxWXy2"
      },
      "outputs": [],
      "source": [
        "data.duplicated(). sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of colums & rows in the dataset\n",
        "print('The dataset has {} rows and {} colums'.format(data.shape[0], data.shape[1]))"
      ],
      "metadata": {
        "id": "Tj51uGpqbUyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtNEmlk1WXy3"
      },
      "outputs": [],
      "source": [
        "# Get the value counts for the 'airline_sentiment' column\n",
        "sentiment_counts = data['airline_sentiment'].value_counts()\n",
        "\n",
        "# Plot the value counts using seaborn\n",
        "sns.set_style (\"darkgrid\")\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGj5Hf25WXy3"
      },
      "source": [
        "# Suitable data preparation steps for NLP data\n",
        "a. Html tag removal.\n",
        "\n",
        "b. Tokenization.\n",
        "\n",
        "c. Remove the numbers.\n",
        "\n",
        "d. Removal of Special Characters and Punctuations.\n",
        "\n",
        "e. Conversion to lowercase.\n",
        "\n",
        "f. Lemmatize or stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_html_cleanup( raw_review ):\n",
        "  # 1. Remove HTML\n",
        "  review_text = BeautifulSoup(raw_review).get_text()\n",
        "  return review_text"
      ],
      "metadata": {
        "id": "DGGk3hUuBchg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_contractions(raw_review):\n",
        "    #Replace contractions in raw_review\n",
        "    return contractions.fix(raw_review)"
      ],
      "metadata": {
        "id": "N45wwqjKBfSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_tokenization( raw_review ):\n",
        "  # 2. Perform Tokenization\n",
        "  word_tokens = word_tokenize(raw_review)  # Tokenization\n",
        "  return word_tokens"
      ],
      "metadata": {
        "id": "JSBbFn3iBfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(list_of_words): \n",
        "    pattern = '[0-9]'\n",
        "    list = [re.sub(pattern, '', i) for i in list_of_words] \n",
        "    return list"
      ],
      "metadata": {
        "id": "PftNOTo2BfKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_character_punctuation(list_of_words): \n",
        "    pattern = '[^A-Za-z0-9]+'\n",
        "    list = [re.sub(pattern, '', i) for i in list_of_words] \n",
        "    return list"
      ],
      "metadata": {
        "id": "bAXm92anBfHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []                        # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)    # Append processed words to new list.\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "EkwzLeMeBfD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []                        # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        new_word = word.lower()           # Converting to lowercase\n",
        "        new_words.append(new_word)        # Append processed words to new list.\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "k-jcfs_uBpAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_empty_string(words):\n",
        "  return list(filter(None, words))"
      ],
      "metadata": {
        "id": "w4icy-ZYBo9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []                            # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)                # Append processed words to new list.\n",
        "    return stems"
      ],
      "metadata": {
        "id": "BsKRbx7lBo56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []                           # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)              # Append processed words to new list.\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "60NC7kwkBt8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_cleanup(raw_review):\n",
        "    clean_review = perform_html_cleanup(raw_review)\n",
        "    clean_review = replace_contractions(clean_review)\n",
        "    clean_review = perform_tokenization(clean_review)\n",
        "    clean_review = remove_numbers(clean_review)\n",
        "    clean_review = remove_special_character_punctuation(clean_review)\n",
        "    clean_review = remove_punctuation(clean_review)\n",
        "    clean_review  = to_lowercase(clean_review)\n",
        "    clean_review = remove_empty_string(clean_review)\n",
        "    #clean_review = stem_words(clean_review)\n",
        "    clean_review = lemmatize_verbs(clean_review)\n",
        "    return clean_review"
      ],
      "metadata": {
        "id": "MZ-bx8buBwua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "id": "8AwQz1-OBy_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "VskvBFKuCwVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_reviews = []\n",
        "\n",
        "for i, row in data.iterrows():\n",
        "    words = data.at[i, 'text']\n",
        "    words = perform_cleanup(words)\n",
        "    data.at[i,'text'] = \" \".join( words )\n",
        "    clean_reviews.append( data.at[i, 'text'] )\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-aspr8DAB4gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the data\n",
        "def preprocess_text(string):\n",
        "    \"\"\"preprocess the text in a string\n",
        "\n",
        "    Args:\n",
        "        string (str): string you want to preprocess\n",
        "    \"\"\"\n",
        "    # lower case the text\n",
        "    string = string.lower()\n",
        "    # remove the apostrophe and replace it with a space\n",
        "    string = string.replace(\"'\", \" \")\n",
        "    # remove the backslash and replace it with a space\n",
        "    string = string.replace(\"\\\\\", \" \")\n",
        "    # remove the @ and replace it with a space\n",
        "    string = string.replace(\"@\", \" \")\n",
        "    # remove the special characters by only allowing letters\n",
        "    # this uses a regular expression (regex) which is a special syntax to define a pattern to search for\n",
        "    string = re.sub(r\"[^a-zA-Z]\", \" \", string)\n",
        "\n",
        "    return string\n",
        "\n",
        "data['text'] = data['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "VUHNB7niPONw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "xJp8F4U9hINg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJHm1lICWXy6"
      },
      "source": [
        "### 2.2 Vectorization and Features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Creating the bag of words...\\n\")\n",
        "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
        "# bag of words tool.  \n",
        "count_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
        "                             tokenizer = None,    \\\n",
        "                             preprocessor = None, \\\n",
        "                             stop_words = None,   \\\n",
        "                             max_features = 5000) \n",
        "\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of \n",
        "# strings.\n",
        "count_vectorizer_data_features = count_vectorizer.fit_transform(clean_reviews)\n",
        "\n",
        "# Numpy arrays are easy to work with, so convert the result to an \n",
        "# array\n",
        "count_vectorizer_data_features = count_vectorizer_data_features.toarray()"
      ],
      "metadata": {
        "id": "vHXk3OO2DQHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (count_vectorizer_data_features.shape)\n",
        "print(count_vectorizer_data_features)"
      ],
      "metadata": {
        "id": "WQ00XtUDDTzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the words in the vocabulary\n",
        "count_vectorizer_vocab = count_vectorizer.get_feature_names()\n",
        "print (count_vectorizer_vocab)"
      ],
      "metadata": {
        "id": "2KJ0dtdPDdix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer_stop_words = count_vectorizer.get_stop_words()\n",
        "print (count_vectorizer_stop_words)\n",
        "# There are no stop words since we are doing sentiment analysis"
      ],
      "metadata": {
        "id": "l62_ZxVfDigh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum up the counts of each vocabulary word\n",
        "dist = np.sum(count_vectorizer_data_features, axis=0)\n",
        "\n",
        "# For each, print the vocabulary word and the number of times it \n",
        "# appears in the training set\n",
        "for tag, count in zip(count_vectorizer_vocab, dist):\n",
        "    print (count, tag)"
      ],
      "metadata": {
        "id": "PU1JMB7-DmZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the \"TfidfVectorizer\" object\n",
        "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
        "                             tokenizer = None,    \\\n",
        "                             preprocessor = None, \\\n",
        "                             stop_words = None,   \\\n",
        "                             max_features = 5000,\n",
        "                             min_df=5, \n",
        "                             max_df=0.7,\n",
        "                             ngram_range=(1,2)) \n",
        "\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of \n",
        "# strings.\n",
        "tfidf_vectorizer_data_features = tfidf_vectorizer.fit_transform(clean_reviews)\n",
        "\n",
        "# Numpy arrays are easy to work with, so convert the result to an \n",
        "# array\n",
        "tfidf_vectorizer_data_features = tfidf_vectorizer_data_features.toarray()"
      ],
      "metadata": {
        "id": "KNjOAp9aDq9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (tfidf_vectorizer_data_features.shape)\n",
        "print(tfidf_vectorizer_data_features)"
      ],
      "metadata": {
        "id": "34l84M97Du0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the words in the vocabulary\n",
        "tfidf_vectorizer_vocab = tfidf_vectorizer.get_feature_names()\n",
        "print (tfidf_vectorizer_vocab)"
      ],
      "metadata": {
        "id": "9krfRdmzDxsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer_stop_words = tfidf_vectorizer.get_stop_words()\n",
        "print (tfidf_vectorizer_stop_words)"
      ],
      "metadata": {
        "id": "04R9GdniD1Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum up the counts of each vocabulary word\n",
        "tf_df_dist = np.sum(tfidf_vectorizer_data_features, axis=0)\n",
        "\n",
        "# For each, print the vocabulary word and the number of times it \n",
        "# appears in the training set\n",
        "for tag, count in zip(tfidf_vectorizer_vocab, tf_df_dist):\n",
        "    print (count, tag)"
      ],
      "metadata": {
        "id": "1vym9uMED4cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "BjiOzAqzO2TU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVhuHP1uWXy_"
      },
      "source": [
        "# Splitting of the data using a sensible split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset into 80% train and 20% test\n",
        "train_data = data[:int(0.8*data.shape[0])]\n",
        "test_data = data[int(0.8*data.shape[0]):]\n",
        "\n",
        "print(train_data.head(10))\n",
        "print(test_data.head(10))\n",
        "\n",
        "# Automatic Test to check whether your split was correct\n",
        "assert train_data.shape[0] >= 8997 and train_data.shape[0] <= 8998\n",
        "assert test_data.shape[0] >= 2249 and test_data.shape[0] <= 2250 "
      ],
      "metadata": {
        "id": "jJTSyF2sAas0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the numpy random seed to 42 to avoid bais\n",
        "np.random.seed(42)\n",
        "\n",
        "#Shuffle data\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "print(data.head())\n",
        "\n",
        "print(train_data.head(10))\n",
        "print(test_data.head(10))"
      ],
      "metadata": {
        "id": "PTmhso4WGpNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Extract the target column from the train and test dataframes and save them in the variables y_train and y_test\n",
        "# TODO: Drop the target column from the train and test dataframes and save the remaining dataframes in the variables X_train and X_test\n",
        "y_train = data['airline_sentiment']\n",
        "y_test = data['airline_sentiment']\n",
        "X_train = data.drop(columns=['airline_sentiment'])\n",
        "X_test = data.drop(columns=['airline_sentiment'])"
      ],
      "metadata": {
        "id": "apPAsft9Jo7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the features and labels as above and name them  in variables X and y, dropping the target colum 'cnt' to be predicted\n",
        "X = data.drop(columns=['airline_sentiment'])\n",
        "y = data['airline_sentiment']\n",
        "\n",
        "# shuffle and extract an 80/20 split of the data in the dataframe df using sklearn.model_selection.train_test_split\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# printing out the top few lines of each dataframe to check that the split was correct\n",
        "print(X_train.head())\n",
        "print(X_test.head())\n",
        "print(y_train.head())\n",
        "print(y_test.head())\n",
        "\n",
        "# Automatic Test to check whether your split was correct\n",
        "assert train_data.shape[0] >= 8997 and train_data.shape[0] <= 8998\n",
        "assert test_data.shape[0] >= 2249 and test_data.shape[0] <= 2250 "
      ],
      "metadata": {
        "id": "RpFBxJsCGu-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "X_test_vectors = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "jAp9-qS3QOQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "sYmcKdvEiM6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "IKrXzFY1jsvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "tokens = []\n",
        "for tweet in data[\"text\"]:\n",
        "    tokens.append(nltk.word_tokenize(tweet))\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Fg4FEYj1iGKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"text\"] = tokens\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "GlKW84MyjSIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_list = data[\"text\"].tolist()\n",
        "test_data_list = data[\"text\"].tolist()"
      ],
      "metadata": {
        "id": "WJr9usovqSl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(train_data_list):\n",
        "    \"\"\"removes stopwords from a list of strings by simply checking on a word for word basis if the word is a stopword and if it is, we are removing it\n",
        "\n",
        "    Args:\n",
        "        data_list (list): list of strings you want to remove stopwords from\n",
        "    \"\"\"\n",
        "    # download the stopwords\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "    # Convert the elements of the input list to strings if they are not already strings\n",
        "    train_data_list = [str(x) for x in train_data_list]\n",
        "\n",
        "    # Get the list of stopwords\n",
        "    stopword_list = stopwords.words(\"english\")\n",
        "\n",
        "    # Remove stopwords from the list of strings\n",
        "    for i in range(len(train_data_list)):\n",
        "        train_data_list[i] = \" \".join(\n",
        "            [word for word in train_data_list[i].split() if word not in (stopword_list)]\n",
        "        )\n",
        "    return train_data_list\n"
      ],
      "metadata": {
        "id": "9dFD4lOvqpFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(test_data_list):\n",
        "    \"\"\"removes stopwords from a list of strings by simply checking on a word for word basis if the word is a stopword and if it is, we are removing it\n",
        "\n",
        "    Args:\n",
        "        data_list (list): list of strings you want to remove stopwords from\n",
        "    \"\"\"\n",
        "    # download the stopwords\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "    # Convert the elements of the input list to strings if they are not already strings\n",
        "    test_data_list = [str(x) for x in test_data_list]\n",
        "\n",
        "    # Get the list of stopwords\n",
        "    stopword_list = stopwords.words(\"english\")\n",
        "\n",
        "    # Remove stopwords from the list of strings\n",
        "    for i in range(len(train_data_list)):\n",
        "        test_data_list[i] = \" \".join(\n",
        "            [word for word in test_data_list[i].split() if word not in (stopword_list)]\n",
        "        )\n",
        "    return test_data_list"
      ],
      "metadata": {
        "id": "TJDPv3x3tzUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_list = remove_stopwords(train_data_list)\n",
        "test_data_list = remove_stopwords(test_data_list)"
      ],
      "metadata": {
        "id": "EA8g7rehq6LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit-learn's implementation of the vectorizer also has the option to remove stop words, create n-grams and create lowercase words. We have performed some of those steps in the preprocessing step, so we will not use those options here.\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)\n",
        "\n",
        "# fit on train data\n",
        "vectorizer.fit(train_data_list)\n",
        "train_data_vectorized = vectorizer.transform(train_data_list)\n",
        "\n",
        "vectorizer.fit(test_data_list)\n",
        "test_data_vectorized = vectorizer.transform(test_data_list)\n"
      ],
      "metadata": {
        "id": "jy63BHxBrQva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the top 1000 features\n",
        "selector = SelectKBest(f_classif, k=1000)\n",
        "# fit on train data\n",
        "selector.fit(train_data_vectorized, train_data_list)\n",
        "\n",
        "# vectorizing the data transforms the data into a sparse matrix.\n",
        "# we can use the toarray() method to transform the sparse matrix into a dense matrix. this will make it easier to visualize the data, but this happens at a later stage.\n",
        "train_data_vectorized = selector.transform(train_data_vectorized)\n",
        "test_data_vectorized = selector.transform(test_data_vectorized)\n",
        "\n",
        "# lets just check the shape of the data to see if we have the correct amount of features\n",
        "print(train_data_vectorized.shape)\n",
        "print(test_data_vectorized.shape)"
      ],
      "metadata": {
        "id": "S-zB8emKrfwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the CountVectorizer class\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Vectorize the training data\n",
        "X_train = vectorizer.fit_transform(train_data_list)\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test = vectorizer.transform(test_data_list)\n"
      ],
      "metadata": {
        "id": "8qhnbhzitkkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKUqPv-AWXy_"
      },
      "source": [
        "# Training of ML classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine (SVM:SVC)"
      ],
      "metadata": {
        "id": "19LWoUakcts9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports from sklearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pickle\n",
        "import sklearn.metrics as sm\n",
        "from sklearn import datasets, linear_model, preprocessing\n",
        "from sklearn.metrics import classification_report, confusion_matrix, explained_variance_score, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "x6AuYoccdzWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = min(X_train.shape[0], y_train.shape[0])\n",
        "X_train = X_train[:num_samples, :]\n",
        "y_train = y_train[:num_samples]"
      ],
      "metadata": {
        "id": "0Ok1IDAyEjyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call the model and fit it to the taining data\n",
        "m_SVR = SVC().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "C1w3sRZHcvrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run model prediction\n",
        "y_pred = m_SVR.predict(X_test)"
      ],
      "metadata": {
        "id": "JDLs6wHwee-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_parameters = [\n",
        "    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n",
        "    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]}\n",
        "]\n",
        "model = SVC()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    model, tuned_parameters, scoring=\"accuracy\",cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "MXZXbAV3erUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_pred, y_test,edgecolors=(0, 0, 1))\n",
        "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'y--', lw=3)\n",
        "ax.set_xlabel('Predictions')\n",
        "ax.set_ylabel('Actual Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5h3gNZEKfihI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom deep learning Fully Connected network model built with Keras/Tensorflow"
      ],
      "metadata": {
        "id": "rHJ7QGnvMqvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import relevant modules\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow import keras\n",
        "import time\n"
      ],
      "metadata": {
        "id": "JoEdNt_Uw7KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = train_data_vectorized.shape[1]\n",
        "print(num_features)"
      ],
      "metadata": {
        "id": "40WAmAo67KZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the labels are of value 0 and 1. We need to convert them to one-hot encoded vectors \n",
        "train_data_list_np = np.array(train_data_list)\n",
        "test_data_list_np = np.array(test_data_list)\n",
        "print(test_data_list_np.shape)\n",
        "\n",
        "train_data = tensorflow.keras.utils.to_categorical(\n",
        "    train_data_list_np, num_classes=2)\n",
        "\n",
        "test_data = tensorflow.keras.utils.to_categorical(train_data_list_np, num_classes=2)\n",
        "\n",
        "print(train_data_vectorized.shape, train_data.shape)\n"
      ],
      "metadata": {
        "id": "qv6PuhUA7XwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(keras.layers.Dense(units=32, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=16, activation='relu'))\n",
        "model.add(keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Z3wSruhbzBlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uPCKmrXhzSzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_data_vectorized.toarray(),\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "Al0GeAEPzt1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "x_train = X_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#Define the convolutional layers of the neural network\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "## Flatten the output of the convolutional layers\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "## Add the first fully-connected layer\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "# Add the second fully-connected layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "Dv0AhPz5w8DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0vuIliZWXzA"
      },
      "source": [
        "# Evaluation of ML classifiers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report\n",
        "print(20*\"-\"+ \"Grid Searched SVM\" + 20*\"-\")\n",
        "best_svm = SVC(C=100, degree=3, kernel=\"poly\")\n",
        "best_svm.fit(X_train, y_train)\n",
        "predictions = best_svm.predict(X_test)\n",
        "print(\"\\nPerformance report:\\n\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "print(20*\"-\"+ \"Baseline SVM\" + 20*\"-\")\n",
        "baseline_sv = SVC()\n",
        "baseline_sv.fit(X_train, y_train)\n",
        "predictions = baseline_sv.predict(X_test)\n",
        "print(\"\\nPerformance report:\\n\")\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "B4mJJ6mPAlUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "accuracy = SVM.score(X_test, y_test)\n",
        "print(\"Test accuracy: {:.2f}\".format(accuracy))"
      ],
      "metadata": {
        "id": "02l090mjLspD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting the CNN classifier with data augmentation\n",
        "Data augmentaiton aids us in increasing the training dataset size, which is achieved in the following code cells. "
      ],
      "metadata": {
        "id": "ESey6tdfPC1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------"
      ],
      "metadata": {
        "id": "J5_6iUQ0Os9k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FNapqmJAiQx"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmCLDPLGWXzA"
      },
      "source": [
        "--- \n",
        "\n",
        "This cell goes to the very bottom of your submitted notebok.\n",
        "You are requried to link the sources and web-links that you have used for various parts of this coursework. \n",
        "\n",
        "Write them sources used in the following format similar to the first examle in the sources list below :\n",
        "\n",
        "    - what you have used them for : web-link\n",
        "\n",
        "Sources:\n",
        "\n",
        "- Implement NLP Data Preprocessing : https://www.kaggle.com/code/prasadmenonsrees/project-nlp-sentiment-analysis-twitter-us-air\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.15 ('LabEnvDeepLearning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "26db5689d574289e0aeada50c74b5d0a9f8ada37a3c0adb130ce6ce79dc37c9f"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}